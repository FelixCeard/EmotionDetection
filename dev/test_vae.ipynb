{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2e35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import EmotionDataset, create_dataloader, create_dataloaders\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# from model import EmotionDetectionModel\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdf3ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CREMA-D...\n",
      "Loading ESD...\n",
      "Loading JL-Corpus...\n",
      "Loading RAVDESS Actors...\n",
      "Loading RAVDESS Speech...\n",
      "Loading SAVE-E...\n",
      "Loading TESS...\n",
      "Loaded all datasets\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "dataset = EmotionDataset(root_dir=Path(os.getenv(\"PATH_DATASETS\")), resample_rate=16_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756a587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 561])\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\feature\\spectral.py:2148: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "mfccs, label = dataset[0]\n",
    "\n",
    "# print(X.shape)\n",
    "print(mfccs.shape)\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a28961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CREMA-D...\n",
      "Loading ESD...\n",
      "Loading JL-Corpus...\n",
      "Loading RAVDESS Actors...\n",
      "Loading RAVDESS Speech...\n",
      "Loading SAVE-E...\n",
      "Loading TESS...\n",
      "Loaded all datasets\n",
      "Loading CREMA-D...\n",
      "Loading ESD...\n",
      "Loading JL-Corpus...\n",
      "Loading RAVDESS Actors...\n",
      "Loading RAVDESS Speech...\n",
      "Loading SAVE-E...\n",
      "Loading TESS...\n",
      "Loaded all datasets\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = create_dataloaders(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799b27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Detector(nn.Module):\n",
    "\n",
    "    def __init__(self, num_mfccs_features=20, num_classes=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_mfccs_features, out_channels=32, kernel_size=5, stride=2),\n",
    "            # nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "            # nn.MaxPool1d(kernel_size=5, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(in_features=64, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.feature_extractor(x)\n",
    "        # print(x.shape)\n",
    "        x = x.mean(dim=2)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328c9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2441588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 561])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ddd5a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Detector(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv1d(20, 32, kernel_size=(5,), stride=(2,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
       "    (4): ELU(alpha=1.0)\n",
       "    (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = Detector()\n",
    "detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b30aa790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20, 1793]), torch.Size([32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(train_loader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15946f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5725e-03, -8.5675e-02, -1.1811e-01,  5.0820e-02, -2.0206e-02,\n",
       "         -5.4146e-02,  4.4480e-02, -9.1047e-02],\n",
       "        [ 1.7811e-02, -2.7120e-02, -5.3571e-02, -1.7111e-02, -7.5288e-02,\n",
       "         -1.2967e-01,  8.1670e-02, -1.2324e-01],\n",
       "        [-2.9055e-02, -9.6368e-02, -1.4586e-01,  6.0536e-02, -2.0108e-02,\n",
       "         -4.6663e-02,  2.6467e-02, -5.5843e-02],\n",
       "        [-5.6705e-02, -1.5930e-01, -2.1151e-01,  1.3603e-01,  2.7248e-02,\n",
       "          5.7586e-02,  2.3162e-02,  3.5275e-03],\n",
       "        [ 2.2097e-02,  2.1194e-03,  7.1377e-03, -8.0477e-02, -1.1444e-01,\n",
       "         -1.9384e-01,  1.0790e-01, -1.6161e-01],\n",
       "        [-5.5685e-02, -1.4074e-01, -1.8807e-01,  1.0928e-01,  1.0624e-02,\n",
       "          1.4930e-02,  3.0819e-02, -1.6660e-02],\n",
       "        [-3.3237e-02, -8.9647e-02, -1.3206e-01,  4.3108e-02, -3.6633e-02,\n",
       "         -4.7378e-02,  3.4611e-02, -4.2589e-02],\n",
       "        [-9.6291e-02, -1.8764e-01, -2.5750e-01,  1.7463e-01,  5.2887e-02,\n",
       "          6.0823e-02, -9.8112e-03,  3.7692e-02],\n",
       "        [ 3.6649e-02,  1.5663e-02,  3.0411e-02, -7.3983e-02, -9.5774e-02,\n",
       "         -2.1473e-01,  1.2217e-01, -2.0205e-01],\n",
       "        [-8.1791e-03, -6.9612e-02, -1.1672e-01,  2.9879e-02, -3.4203e-02,\n",
       "         -6.8281e-02,  2.3552e-02, -6.3066e-02],\n",
       "        [-4.9599e-02, -1.4269e-01, -2.0221e-01,  1.2630e-01,  2.3244e-02,\n",
       "          1.5413e-02,  5.6007e-03, -1.2354e-02],\n",
       "        [-8.4489e-02, -1.8175e-01, -2.5296e-01,  1.6463e-01,  4.3875e-02,\n",
       "          8.0560e-02, -4.5769e-03,  3.8788e-02],\n",
       "        [ 5.4070e-02,  6.9940e-03, -1.7618e-02, -5.5001e-02, -8.2559e-02,\n",
       "         -1.7752e-01,  5.6165e-02, -1.5002e-01],\n",
       "        [-2.0370e-02, -9.0975e-02, -1.1423e-01,  5.4856e-02, -2.7622e-02,\n",
       "         -4.3718e-02,  6.1693e-02, -7.1879e-02],\n",
       "        [ 6.1817e-02,  2.4726e-02,  4.5229e-02, -8.5453e-02, -1.1297e-01,\n",
       "         -2.0827e-01,  1.3473e-01, -2.2066e-01],\n",
       "        [-1.7002e-02, -5.5405e-02, -5.4760e-02,  2.3021e-02, -4.9782e-02,\n",
       "         -1.1954e-01,  1.0509e-01, -1.2353e-01],\n",
       "        [ 7.1777e-03, -9.2690e-04, -1.4613e-02, -9.7804e-02, -1.2050e-01,\n",
       "         -1.7390e-01,  8.9023e-02, -1.2836e-01],\n",
       "        [ 3.0551e-02, -2.6402e-02, -2.4559e-02, -2.3246e-02, -7.4991e-02,\n",
       "         -1.3957e-01,  8.1046e-02, -1.4513e-01],\n",
       "        [-3.1057e-02, -1.0029e-01, -1.3395e-01,  8.4242e-02, -1.1896e-02,\n",
       "         -4.7661e-02,  5.7740e-02, -5.5103e-02],\n",
       "        [-5.2396e-02, -1.1466e-01, -1.6850e-01,  8.1999e-02, -8.3370e-03,\n",
       "         -3.2122e-02,  9.6831e-03, -3.2751e-02],\n",
       "        [ 1.7165e-03, -4.7395e-02, -6.4081e-02, -7.9673e-05, -5.1673e-02,\n",
       "         -1.1885e-01,  6.9535e-02, -1.2278e-01],\n",
       "        [-4.7049e-02, -1.2902e-01, -1.6246e-01,  9.0246e-02,  2.3849e-03,\n",
       "         -6.4693e-03,  5.6413e-02, -4.6281e-02],\n",
       "        [-7.2839e-02, -1.4142e-01, -1.6787e-01,  9.7543e-02,  1.0430e-03,\n",
       "          1.0972e-02,  5.2557e-02, -2.2821e-02],\n",
       "        [ 1.8246e-02, -4.3842e-02, -8.6104e-02,  4.4888e-03, -4.6012e-02,\n",
       "         -1.0498e-01,  1.0441e-02, -9.5979e-02],\n",
       "        [ 7.7886e-02,  5.1616e-02,  3.2429e-02, -1.2579e-01, -1.3081e-01,\n",
       "         -2.0867e-01,  5.0577e-02, -1.6001e-01],\n",
       "        [ 7.3427e-02,  3.5021e-02,  1.6496e-02, -9.3152e-02, -1.1127e-01,\n",
       "         -2.1401e-01,  6.0687e-02, -1.6530e-01],\n",
       "        [-9.2003e-03, -9.9549e-02, -1.3471e-01,  6.0364e-02, -1.2706e-02,\n",
       "         -2.6831e-02,  4.9559e-02, -6.3102e-02],\n",
       "        [-2.2365e-02, -9.9322e-02, -1.5091e-01,  7.1143e-02, -1.3049e-02,\n",
       "         -4.0367e-02,  2.3715e-02, -4.4485e-02],\n",
       "        [-4.2328e-02, -1.0025e-01, -1.3302e-01,  6.1228e-02, -2.0744e-02,\n",
       "         -4.0283e-02,  5.9651e-02, -5.1891e-02],\n",
       "        [ 4.4771e-04, -5.7606e-02, -8.2647e-02,  3.3475e-03, -5.2641e-02,\n",
       "         -8.1165e-02,  5.8253e-02, -9.0539e-02],\n",
       "        [-9.6492e-03, -9.2254e-02, -1.3545e-01,  6.8151e-02, -1.0815e-02,\n",
       "         -4.7248e-02,  3.1299e-02, -6.7372e-02],\n",
       "        [-2.0119e-02, -5.7067e-02, -9.6526e-02,  2.1192e-02, -4.6175e-02,\n",
       "         -1.0084e-01,  4.5498e-02, -8.1796e-02]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd947dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of the model: 22,376\n"
     ]
    }
   ],
   "source": [
    "# number of parameters of the model\n",
    "print(f'Number of parameters of the model: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e94e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import random\n",
    "\n",
    "\n",
    "class EmotionDataset2(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, resample_rate=16_000, n_mfcc=20, n_fft=64):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.resample_rate = resample_rate\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "\n",
    "        self.audio_files = []\n",
    "        self.labels = []\n",
    "        # the labels are:\n",
    "        # 1: anger\n",
    "        # 2: disgust\n",
    "        # 3: fear\n",
    "        # 4: happy\n",
    "        # 5: neutral\n",
    "        # 6: sad\n",
    "        # 7: surprise\n",
    "        \n",
    "        self.dataset_source = []\n",
    "        # the dataset source is:\n",
    "        # 1: crema_d\n",
    "        # 2: esd\n",
    "        # 3: jl_corpus\n",
    "        # 4: ravdess - Actors\n",
    "        # 5: ravdess - Speech\n",
    "\n",
    "        self.indices = None\n",
    "\n",
    "        self.setup_datasets()\n",
    "\n",
    "    def setup_datasets(self):\n",
    "        # read the different datasets and save the audio files and labels\n",
    "\n",
    "        # ? CREMA-D\n",
    "        print('Loading CREMA-D...')\n",
    "        crema_d_path = self.root_dir / \"CREMA-D\" / \"AudioWAV\"\n",
    "        crema_d_files = crema_d_path.glob(\"*.wav\")\n",
    "        crema_d_conversion_dict = {\n",
    "            \"ANG\": 1,\n",
    "            \"DIS\": 2,\n",
    "            \"FEA\": 3,\n",
    "            \"HAP\": 4,\n",
    "            \"NEU\": 5,\n",
    "            \"SAD\": 6,\n",
    "            # \"SUR\": 7\n",
    "        }\n",
    "\n",
    "\n",
    "        for file in crema_d_files:\n",
    "            label = file.stem.split(\"_\")[-2]\n",
    "            label = crema_d_conversion_dict[label]\n",
    "            \n",
    "            self.audio_files.append(file)\n",
    "            self.labels.append(label)\n",
    "            self.dataset_source.append(1)\n",
    "\n",
    "        \n",
    "        # ? ESD\n",
    "        print('Loading ESD...')\n",
    "        esd_path = self.root_dir / \"ESD\"\n",
    "        esd_conversion_dict = {\n",
    "            \"Angry\": 1,\n",
    "            # \"Discust\": 2,\n",
    "            # \"Fear\": 3,\n",
    "            \"Happy\": 4,\n",
    "            \"Neutral\": 5,\n",
    "            \"Sad\": 6,\n",
    "            \"Surprise\": 7\n",
    "        }\n",
    "\n",
    "        \n",
    "        for speaker in esd_path.iterdir():\n",
    "            if not speaker.is_dir():\n",
    "                continue\n",
    "            for emotion in speaker.iterdir():\n",
    "                if not emotion.is_dir():\n",
    "                    continue\n",
    "                for file in emotion.glob(\"*.wav\"):\n",
    "                    label = esd_conversion_dict[emotion.name]\n",
    "\n",
    "                    self.audio_files.append(file)\n",
    "                    self.labels.append(label)\n",
    "                    self.dataset_source.append(2)\n",
    "\n",
    "\n",
    "        # ? JL-Corpus\n",
    "        print('Loading JL-Corpus...')\n",
    "        jl_corpus_path = self.root_dir / \"JL-Corpus\" / 'Raw JL corpus (unchecked and unannotated)' / 'JL(wav+txt)'\n",
    "        files = jl_corpus_path.glob(\"*.wav\")\n",
    "        jl_corpus_conversion_dict = {\n",
    "            \"angry\": 1,\n",
    "            # \"discust\": 2,\n",
    "            # \"fear\": 3,\n",
    "            \"happy\": 4,\n",
    "            \"neutral\": 5,\n",
    "            \"sad\": 6,\n",
    "            \"surprise\": 7,\n",
    "            \"anxious\": 3, # we use it as fear here\n",
    "            \"apologetic\": None,\n",
    "            \"assertive\": None, \n",
    "            \"concerned\": 3, # we use it as fear here\n",
    "            \"encouraging\": None, \n",
    "            \"excited\": 4,\n",
    "        }\n",
    "\n",
    "        for file in files:\n",
    "            label = file.stem.split(\"_\")[1]\n",
    "            label = jl_corpus_conversion_dict[label]\n",
    "            if label is None:\n",
    "                continue # skip the file\n",
    "\n",
    "            self.audio_files.append(file)\n",
    "            self.labels.append(label)\n",
    "            self.dataset_source.append(3)\n",
    "\n",
    "\n",
    "        # ? RAVDESS Actors\n",
    "        print('Loading RAVDESS Actors...')\n",
    "        ravdess_actors_path = self.root_dir / \"RAVDESS\" / \"actors\"\n",
    "        ravdess_conversion_dict = {\n",
    "            \"01\": 5,\n",
    "            \"02\": 5, # is neutral too\n",
    "            \"03\": 4,\n",
    "            \"04\": 6,\n",
    "            \"05\": 1,\n",
    "            \"06\": 3,\n",
    "            \"07\": 2,\n",
    "            \"08\": 7\n",
    "        }\n",
    "\n",
    "        for actor in ravdess_actors_path.iterdir():\n",
    "            if not actor.is_dir():\n",
    "                continue\n",
    "            for file in actor.glob(\"*.wav\"):\n",
    "                \n",
    "                label = file.stem.split(\"-\")[2]\n",
    "                label = ravdess_conversion_dict[label]\n",
    "\n",
    "                self.audio_files.append(file)\n",
    "                self.labels.append(label)\n",
    "                self.dataset_source.append(4)\n",
    "\n",
    "\n",
    "        # ? RAVDESS Speech\n",
    "        print('Loading RAVDESS Speech...')\n",
    "        ravdess_speech_path = self.root_dir / \"RAVDESS\" / \"speech\"\n",
    "        ravdess_conversion_dict = {\n",
    "            \"01\": 5,\n",
    "            \"02\": 5, # is neutral too\n",
    "            \"03\": 4,\n",
    "            \"04\": 6,\n",
    "            \"05\": 1,\n",
    "            \"06\": 3,\n",
    "            \"07\": 2,\n",
    "            \"08\": 7\n",
    "        }\n",
    "\n",
    "        for actor in ravdess_speech_path.iterdir():\n",
    "            if not actor.is_dir():\n",
    "                continue\n",
    "            for file in actor.glob(\"*.wav\"):\n",
    "                \n",
    "                label = file.stem.split(\"-\")[2]\n",
    "                label = ravdess_conversion_dict[label]\n",
    "\n",
    "                self.audio_files.append(file)\n",
    "                self.labels.append(label)\n",
    "                self.dataset_source.append(4)\n",
    "\n",
    "\n",
    "        # ? SAVE-E\n",
    "        print('Loading SAVE-E...')\n",
    "        savee_path = self.root_dir / \"SAVE-E\" / \"ALL\"\n",
    "        savee_conversion_dict = {\n",
    "            \"a\": 1,\n",
    "            \"d\": 2,\n",
    "            \"f\": 3,\n",
    "            \"h\": 4,\n",
    "            \"n\": 5,\n",
    "            \"sa\": 6,\n",
    "            \"su\": 7\n",
    "        }\n",
    "\n",
    "        for file in savee_path.glob(\"*.wav\"):\n",
    "            label = re.findall(r'([a-z]+)\\d+', file.stem)[0]\n",
    "            label = savee_conversion_dict[label]\n",
    "\n",
    "            self.audio_files.append(file)\n",
    "            self.labels.append(label)\n",
    "            self.dataset_source.append(5)\n",
    "\n",
    "        # ? TESS\n",
    "        print('Loading TESS...')\n",
    "        tess_path = self.root_dir / \"TESS\" / \"TESS Toronto emotional speech set data\"\n",
    "        tess_conversion_dict = {\n",
    "            \"angry\": 1,\n",
    "            \"disgust\": 2,\n",
    "            \"fear\": 3,\n",
    "            \"happy\": 4,\n",
    "            \"neutral\": 5,\n",
    "            \"sad\": 6,\n",
    "            \"pleasant_surprise\": 7,\n",
    "            \"surprise\": 7,\n",
    "            \"surprised\": 7\n",
    "        }\n",
    "\n",
    "        for emotion in tess_path.iterdir():\n",
    "            if not emotion.is_dir():\n",
    "                continue\n",
    "            if emotion.name == \"TESS Toronto emotional speech set data\":\n",
    "                continue\n",
    "\n",
    "            label = emotion.name.split(\"_\")[-1]\n",
    "            label = tess_conversion_dict[label.lower()]\n",
    "\n",
    "            for file in emotion.glob(\"*.wav\"):\n",
    "                self.audio_files.append(file)\n",
    "                self.labels.append(label)\n",
    "                self.dataset_source.append(6)\n",
    "\n",
    "        print('Loaded all datasets')\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.indices is None:\n",
    "            return len(self.audio_files)\n",
    "        else:\n",
    "            return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.indices is not None:\n",
    "            idx = self.indices[idx]\n",
    "        audio_file = self.audio_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        dataset_source = self.dataset_source[idx]\n",
    "\n",
    "        # Use soundfile to read the audio file as a workaround for torchaudio backend issues\n",
    "        X, sr = librosa.load(str(audio_file), sr=self.resample_rate, mono=True)\n",
    "\n",
    "        # remove the silences\n",
    "        X, _ = librosa.effects.trim(X, top_db=20)\n",
    "       \n",
    "        mfccs = librosa.feature.mfcc(y=X, sr=sr, n_mfcc=self.n_mfcc, n_fft=self.n_fft, hop_length=self.n_fft//2)\n",
    "\n",
    "        # to tensor\n",
    "        mfccs = torch.tensor(mfccs, dtype=torch.float32)\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=X, sr=sr, n_fft=self.n_fft, hop_length=self.n_fft//2)\n",
    "        mel_spec = torch.tensor(mel_spec, dtype=torch.float32)\n",
    "\n",
    "        chroma = librosa.feature.chroma_stft(y=X, sr=sr, n_fft=self.n_fft, hop_length=self.n_fft//2)\n",
    "        chroma = torch.tensor(chroma, dtype=torch.float32)\n",
    "\n",
    "        return mfccs, mel_spec, chroma, label\n",
    "    \n",
    "    def set_indices(self, indices):\n",
    "        self.indices = indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b2daf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CREMA-D...\n",
      "Loading ESD...\n",
      "Loading JL-Corpus...\n",
      "Loading RAVDESS Actors...\n",
      "Loading RAVDESS Speech...\n",
      "Loading SAVE-E...\n",
      "Loading TESS...\n",
      "Loaded all datasets\n"
     ]
    }
   ],
   "source": [
    "dataset2 = EmotionDataset2(root_dir=Path(os.getenv(\"PATH_DATASETS\")), resample_rate=16_000, n_mfcc=40, n_fft=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17a06647",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs, mel_spec, chroma, label = dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed07ea48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40, 281]), torch.Size([128, 281]), torch.Size([12, 281]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs.shape, mel_spec.shape, chroma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7ed9a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([180, 281])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([mfccs, mel_spec, chroma], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5a98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
